"""
AI Provider ê´€ë¦¬ ëª¨ë“ˆ
Groqì™€ Ollamaë¥¼ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê´€ë¦¬í•©ë‹ˆë‹¤.
"""
 
import logging
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any, List
import asyncio
import openai
import groq
import httpx

from config import config

logger = logging.getLogger(__name__)

class AIProvider(ABC):
    """AI Provider ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    async def generate_response(self, messages: List[Dict[str, Any]], tools: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]:
        """ë©”ì‹œì§€ì™€ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        """Providerê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        pass

class GroqProvider(AIProvider):
    """Groq AI Provider"""
    
    def __init__(self):
        self.client = None
        self.model = config.GROQ_MODEL
    
    def _initialize_client(self):
        """Groq í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        try:
            if not config.GROQ_API_KEY:
                logger.error("Groq API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return
            
            self.client = groq.Groq(api_key=config.GROQ_API_KEY)
            logger.info(f"Groq í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸: {self.model}")
        except Exception as e:
            logger.error(f"Groq í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def constructor(self):
        """ì™¸ë¶€ì—ì„œ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” ì´ˆê¸°í™” ë©”ì„œë“œì…ë‹ˆë‹¤."""
        self._initialize_client()
    
    async def generate_response(self, messages: List[Dict[str, Any]], tools: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]:
        """Groqë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.client:
            return {"error": "Groq í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        try:
            # ê¸°ë³¸ ìš”ì²­ ë°ì´í„°
            request_data = {
                "model": self.model,
                "messages": messages,
                "max_tokens": 8192,
                "temperature": 0.1,
                "reasoning_format": "hidden"
            }
            
            # Toolì´ ìˆìœ¼ë©´ ì¶”ê°€
            if tools:
                request_data["tools"] = tools
                request_data["tool_choice"] = "auto"
            
            # httpxë¥¼ ì‚¬ìš©í•˜ì—¬ timeout ì„¤ì •
            async with httpx.AsyncClient(timeout=180.0) as client:
                response = await client.post(
                    "https://api.groq.com/openai/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {config.GROQ_API_KEY}",
                        "Content-Type": "application/json"
                    },
                    json=request_data
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return result["choices"][0]["message"]
                else:
                    return {"error": f"Groq API ì˜¤ë¥˜: {response.status_code} - {response.text}"}
        except Exception as e:
            logger.error(f"Groq ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"error": f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"}
    
    def is_available(self) -> bool:
        """Groqê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        return self.client is not None and config.GROQ_API_KEY is not None

class OllamaProvider(AIProvider):
    """Ollama AI Provider"""
    
    def __init__(self):
        self.url = config.OLLAMA_URL
        self.model = config.OLLAMA_MODEL
    
    def _initialize_client(self):
        """Ollama í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        try:
            logger.info(f"Ollama í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸: {self.model}")
        except Exception as e:
            logger.error(f"Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def constructor(self):
        """ì™¸ë¶€ì—ì„œ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” ì´ˆê¸°í™” ë©”ì„œë“œì…ë‹ˆë‹¤."""
        self._initialize_client()
    
    async def generate_response(self, messages: List[Dict[str, Any]], tools: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]:
        """Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # ê¸°ë³¸ ìš”ì²­ ë°ì´í„°
            payload = {
                "model": self.model,
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "num_predict": 4096
                }
            }
            
            # Toolì´ ìˆìœ¼ë©´ /api/chat, ì—†ìœ¼ë©´ /api/generate ì‚¬ìš©
            if tools:
                payload["tools"] = tools
                endpoint = "/api/chat"
            else:
                # ê¸°ì¡´ ë°©ì‹ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ systemê³¼ user ë©”ì‹œì§€ë¥¼ í•˜ë‚˜ì˜ promptë¡œ ê²°í•©
                if messages and len(messages) > 0:
                    prompt_parts = []
                    for message in messages:
                        if message.get("role") == "system":
                            prompt_parts.append(f"ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­:\n{message.get('content', '')}")
                        elif message.get("role") == "user":
                            prompt_parts.append(f"ì‚¬ìš©ì ì§ˆë¬¸:\n{message.get('content', '')}")
                    
                    if prompt_parts:
                        payload["prompt"] = "\n\n".join(prompt_parts)
                        del payload["messages"]
                endpoint = "/api/generate"
            
            logger.debug(f"Ollama API í˜¸ì¶œ ì‹œì‘: {self.url}{endpoint}")
            logger.debug(f"ëª¨ë¸: {self.model}")
            
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{self.url}{endpoint}",
                    json=payload,
                    headers={"Content-Type": "application/json"},
                    timeout=300.0
                )
                
                logger.debug(f"Ollama API ì‘ë‹µ ìƒíƒœ: {response.status_code}")
                
                if response.status_code == 200:
                    result = response.json()
                    
                    if tools:
                        # Tool ì‚¬ìš© ë°©ì‹
                        message = result.get("message", {})
                        
                        # UTF-8 ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
                        if "content" in message and isinstance(message["content"], str):
                            content = message["content"]
                            if isinstance(content, bytes):
                                content = content.decode('utf-8', errors='ignore')
                            elif isinstance(content, str):
                                content = content.encode('utf-8', errors='ignore').decode('utf-8')
                            
                            # ì œì–´ ë¬¸ì ì œê±°
                            import re
                            content = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', content)
                            message["content"] = content
                        
                        return message
                    else:
                        # ê¸°ì¡´ ë°©ì‹
                        response_text = result.get("response", "ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤.")
                        
                        # UTF-8 ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
                        try:
                            if isinstance(response_text, bytes):
                                response_text = response_text.decode('utf-8', errors='ignore')
                            elif isinstance(response_text, str):
                                response_text = response_text.encode('utf-8', errors='ignore').decode('utf-8')
                            
                            # ì œì–´ ë¬¸ì ì œê±°
                            import re
                            response_text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', response_text)
                            return {"content": response_text}
                        except Exception as e:
                            logger.error(f"ì‘ë‹µ í…ìŠ¤íŠ¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                            return {"error": "SQL ì¿¼ë¦¬ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}
                else:
                    error_msg = f"Ollama API ì˜¤ë¥˜: {response.status_code} - {response.text}"
                    logger.error(error_msg)
                    return {"error": error_msg}
                    
        except httpx.TimeoutException:
            error_msg = "Ollama API í˜¸ì¶œ ì‹œê°„ ì´ˆê³¼"
            logger.error(error_msg)
            return {"error": error_msg}
        except httpx.ConnectError:
            error_msg = "Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            logger.error(error_msg)
            return {"error": error_msg}
        except Exception as e:
            error_msg = f"Ollama ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}"
            logger.error(error_msg)
            return {"error": error_msg}
    
    def is_available(self) -> bool:
        """Ollamaê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        try:
            
            # ê°„ë‹¨í•œ ì—°ê²° í…ŒìŠ¤íŠ¸
            async def test_connection():
                try:
                    async with httpx.AsyncClient() as client:
                        response = await client.get(f"{self.url}/api/tags", timeout=5.0)
                        if response.status_code == 200:
                            data = response.json()
                            models = data.get("models", [])
                            available_models = [model.get("name", "") for model in models]

                            logger.info(f"\nğŸš¨===== Ollama ëª¨ë¸ ì‹¤í–‰ëª¨ë¸: [{self.model}]")
                            logger.debug(f" Ollama ì‚¬ìš©ê°€ëŠ¥ ëª¨ë¸: \n{available_models}\n ")                               
                            if self.model in available_models:
                                return True
                            else:
                                logger.warning(f"ìš”ì²­í•œ ëª¨ë¸ '{self.model}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                return False
                        else:
                            logger.error(f"Ollama API ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
                            return False
                except Exception as e:
                    logger.error(f"Ollama ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
                    return False
            
            # ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
            try:
                # í˜„ì¬ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ í™•ì¸
                try:
                    loop = asyncio.get_running_loop()
                    # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
                    import concurrent.futures
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(asyncio.run, test_connection())
                        return future.result()
                except RuntimeError:
                    # ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ì§ì ‘ ì‹¤í–‰
                    return asyncio.run(test_connection())
            except Exception as e:
                logger.error(f"ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                return False
                
        except Exception as e:
            logger.error(f"Ollama ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False

class LMStudioProvider(AIProvider):
    """LM Studio AI Provider"""
    
    def __init__(self):
        self.base_url = config.LMSTUDIO_BASE_URL
        self.model = config.LMSTUDIO_MODEL
    
    def _initialize_client(self):
        """LM Studio í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        try:
            logger.info(f"LM Studio í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸: {self.model}")
        except Exception as e:
            logger.error(f"LM Studio í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def constructor(self):
        """ì™¸ë¶€ì—ì„œ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” ì´ˆê¸°í™” ë©”ì„œë“œì…ë‹ˆë‹¤."""
        self._initialize_client()
    
    async def generate_response(self, messages: List[Dict[str, Any]], tools: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]:
        """LM Studioë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # OpenAI í˜¸í™˜ API í˜•ì‹ìœ¼ë¡œ ìš”ì²­
            payload = {
                "model": self.model,
                "messages": messages,
                "max_tokens": 4096,
                "temperature": 0.1,
                "stream": False
            }
            
            # Toolì´ ìˆìœ¼ë©´ ì¶”ê°€
            if tools:
                payload["tools"] = tools
                payload["tool_choice"] = "auto"
            
            logger.debug(f"LM Studio API í˜¸ì¶œ ì‹œì‘: {self.base_url}/chat/completions")
            logger.debug(f"ëª¨ë¸: {self.model}")
            
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{self.base_url}/chat/completions",
                    json=payload,
                    headers={"Content-Type": "application/json"},
                    timeout=300.0
                )
                
                logger.debug(f"LM Studio API ì‘ë‹µ ìƒíƒœ: {response.status_code}")
                
                if response.status_code == 200:
                    result = response.json()
                    
                    if tools:
                        # Tool ì‚¬ìš© ë°©ì‹
                        message = result.get("choices", [{}])[0].get("message", {})
                        
                        # UTF-8 ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
                        if "content" in message and isinstance(message["content"], str):
                            content = message["content"]
                            if isinstance(content, bytes):
                                content = content.decode('utf-8', errors='ignore')
                            elif isinstance(content, str):
                                content = content.encode('utf-8', errors='ignore').decode('utf-8')
                            
                            # ì œì–´ ë¬¸ì ì œê±°
                            import re
                            content = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', content)
                            message["content"] = content
                        
                        return message
                    else:
                        # ê¸°ì¡´ ë°©ì‹
                        response_text = result.get("choices", [{}])[0].get("message", {}).get("content", "ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤.")
                        
                        # UTF-8 ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
                        try:
                            if isinstance(response_text, bytes):
                                response_text = response_text.decode('utf-8', errors='ignore')
                            elif isinstance(response_text, str):
                                response_text = response_text.encode('utf-8', errors='ignore').decode('utf-8')
                            
                            # ì œì–´ ë¬¸ì ì œê±°
                            import re
                            response_text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', response_text)
                            return {"content": response_text}
                        except Exception as e:
                            logger.error(f"ì‘ë‹µ í…ìŠ¤íŠ¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                            return {"error": "SQL ì¿¼ë¦¬ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}
                else:
                    error_msg = f"LM Studio API ì˜¤ë¥˜: {response.status_code} - {response.text}"
                    logger.error(error_msg)
                    return {"error": error_msg}
                    
        except httpx.TimeoutException:
            error_msg = "LM Studio API í˜¸ì¶œ ì‹œê°„ ì´ˆê³¼"
            logger.error(error_msg)
            return {"error": error_msg}
        except httpx.ConnectError:
            error_msg = "LM Studio ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            logger.error(error_msg)
            return {"error": error_msg}
        except Exception as e:
            error_msg = f"LM Studio ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}"
            logger.error(error_msg)
            return {"error": error_msg}
    
    def is_available(self) -> bool:
        """LM Studioê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        try:
            # ê°„ë‹¨í•œ ì—°ê²° í…ŒìŠ¤íŠ¸
            async def test_connection():
                try:
                    async with httpx.AsyncClient() as client:
                        response = await client.get(f"{self.base_url}/models", timeout=5.0)
                        if response.status_code == 200:
                            data = response.json()
                            models = data.get("data", [])
                            available_models = [model.get("id", "") for model in models]
                            logger.info(f"\nğŸš¨===== LM Studio ì‹¤í–‰ëª¨ë¸: [{self.model}]")
                            logger.debug(f"LM Studio ì‚¬ìš©ê°€ëŠ¥ ëª¨ë¸: \n{available_models}\n ")    
                            if self.model in available_models:
                                return True
                            else:
                                logger.warning(f"ìš”ì²­í•œ ëª¨ë¸ '{self.model}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                return False
                        else:
                            logger.error(f"LM Studio API ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
                            return False
                except Exception as e:
                    logger.error(f"LM Studio ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
                    return False
            
            # ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
            try:
                # í˜„ì¬ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ í™•ì¸
                try:
                    loop = asyncio.get_running_loop()
                    # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
                    import concurrent.futures
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(asyncio.run, test_connection())
                        return future.result()
                except RuntimeError:
                    # ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ì§ì ‘ ì‹¤í–‰
                    return asyncio.run(test_connection())
            except Exception as e:
                logger.error(f"ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                return False
                
        except Exception as e:
            logger.error(f"LM Studio ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False

class AIProviderManager:
    """AI Provider ê´€ë¦¬ì"""
    
    def __init__(self):
        self.providers: Dict[str, AIProvider] = {
            "groq": GroqProvider(),
            "ollama": OllamaProvider(),
            "lmstudio": LMStudioProvider()
        }
        self.current_provider = config.AI_PROVIDER.lower()
        # ì´ˆê¸°í™” ì‹œ ìë™ìœ¼ë¡œ constructor í˜¸ì¶œ
        self.constructor()
    
    def constructor(self):
        """AI Providerë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        # í˜„ì¬ ì„¤ì •ëœ Provider ì´ˆê¸°í™”
        current_provider = self.providers.get(self.current_provider)
        if current_provider:
            # Providerì˜ constructor ë©”ì„œë“œ í˜¸ì¶œí•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            current_provider.constructor()
            
            if not current_provider.is_available():
                logger.warning(f"í˜„ì¬ ì„¤ì •ëœ AI Provider '{self.current_provider}'ê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
                self._switch_to_available_provider()
            else:
                logger.info(f"AI Provider '{self.current_provider}'ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” AI Provider: {self.current_provider}")
            self._switch_to_available_provider()
    
    def _switch_to_available_provider(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ Providerë¡œ ì „í™˜í•©ë‹ˆë‹¤."""
        for provider_name, provider in self.providers.items():
            if provider.is_available():
                self.current_provider = provider_name
                logger.info(f"AI Providerê°€ {provider_name}ë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return
        
        logger.error("ì‚¬ìš© ê°€ëŠ¥í•œ AI Providerê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    async def generate_response(self, messages: List[Dict[str, Any]], tools: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]:
        """í˜„ì¬ Providerë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        provider = self.providers.get(self.current_provider)
        if not provider:
            return {"error": "ì‚¬ìš© ê°€ëŠ¥í•œ AI Providerê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        return await provider.generate_response(messages, tools)
    
    def get_current_provider(self) -> str:
        """í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ Provider ì´ë¦„ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.current_provider
    
    def switch_provider(self, provider_name: str) -> bool:
        """Providerë¥¼ ì „í™˜í•©ë‹ˆë‹¤."""
        if provider_name.lower() in self.providers:
            provider = self.providers[provider_name.lower()]
            if provider.is_available():
                self.current_provider = provider_name.lower()
                logger.info(f"AI Providerê°€ {provider_name}ë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return True
            else:
                logger.warning(f"{provider_name} Providerê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
                return False
        else:
            logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” Provider: {provider_name}")
            return False

    def cleanup(self):
        """AI Providerë“¤ì„ ì•ˆì „í•˜ê²Œ ì •ë¦¬í•©ë‹ˆë‹¤."""
        try:
            for provider_name, provider in self.providers.items():
                if hasattr(provider, 'cleanup'):
                    try:
                        provider.cleanup()
                        logger.info(f"{provider_name} Providerê°€ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    except Exception as e:
                        logger.warning(f"{provider_name} Provider ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                else:
                    logger.debug(f"{provider_name} ProviderëŠ” cleanup ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            logger.info("ëª¨ë“  AI Providerê°€ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"AI Provider ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ì „ì—­ AI Provider Manager ì¸ìŠ¤í„´ìŠ¤
ai_manager = AIProviderManager() 